model_class: llama.LlamaForCausalLM # Used to import the model from this class
vocab_size: 128256
hidden_size: 4096
intermediate_size: 14336
num_hidden_layers: 32
num_attention_heads: 32
num_key_value_heads: 8
hidden_act: silu
max_position_embeddings: 131072
bos_token_id: 128000
eos_token_id: 128001
tokenizer_name: meta-llama/Meta-Llama-3-8B
initializer_range: 0.02
rms_norm_eps: 1.0e-05
attention_dropout: false
attention_bias: false
flash_attention: true
rope_theta: 500000.0
fsdp:
  transformer_layer_cls_to_wrap:
    - LlamaDecoderLayer
  xla_fsdp_grad_ckpt: true