name: E2E tests

on:
  push:
    branches:
      - main
  pull_request:
  schedule:
    - cron: "0 8 * * *"  # Run daily at 12AM PST (adjusted for UTC)
  workflow_dispatch:
    inputs:
      docker_url:
        description: If specified, use this PyTorch/XLA base docker image URL instead of the pin.
        required: false
        type: string

jobs:
  tp-run:
    name: Submit workloads
    runs-on: ubuntu-22.04
    env:
      ARTIFACT_DIR: gs://torchprime-e2e-tests/${{ github.job }}/${{ github.run_id }}-${{ github.run_attempt }}
    outputs:
      llama-3-8b-name: ${{ steps.run-llama-3-8b.outputs.name }}
      llama-3_1-8b-sa-name: ${{ steps.run-llama-3_1-8b-SplashAttention.outputs.name }}
      llama-3_1-8b-scan-offload-name: ${{ steps.run-llama-3_1-8b-scan-offload.outputs.name }}
      llama-3-8b-2d-name: ${{ steps.run-llama-3-8b-2d.outputs.name }}
      llama-3-8b-2-slice-name: ${{ steps.run-llama-3-8b-2-slice.outputs.name }}
      mixtral-8x7b-name: ${{ steps.run-mixtral-8x7b.outputs.name }}
      artifact-dir: ${{ steps.artifacts.outputs.artifact_dir }}
    steps:
      - name: Record artifact dir
        id: artifacts
        run: |
          echo "Artifact dir: $ARTIFACT_DIR"
          echo "artifact_dir=$ARTIFACT_DIR" >> "$GITHUB_OUTPUT"
      - name: Maximize build space
        uses: AdityaGarg8/remove-unwanted-software@v4.1
        with:
          remove-dotnet: 'true'
          remove-android: 'true'
          remove-haskell: 'true'
          remove-codeql: 'true'
      - uses: actions/checkout@v4
      - uses: ./.github/actions/e2e-setup
        with:
          gcp_project: ${{ vars.GCP_PROJECT }}
          gcp_zone: ${{ vars.GCP_ZONE }}
          xpk_cluster_name: ${{ vars.XPK_CLUSTER_NAME }}
          tpu_type: ${{ vars.TPU_TYPE }}
          artifact_dir: ${{ env.ARTIFACT_DIR }}
          gcp_sa_key: ${{ secrets.GCP_SA_KEY }}
      - name: Setup Docker URL option
        id: docker-url-option
        run: |
          if [ -n "${{ github.event.inputs.docker_url }}" ]; then
            echo "value=--base-docker-url ${{ github.event.inputs.docker_url }}" >> "$GITHUB_OUTPUT"
          else
            echo "value=" >> "$GITHUB_OUTPUT"
          fi

      # Launch training workloads.

      - name: Run Llama 3.0 8B
        id: run-llama-3-8b
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.1 8B (Splash Attention)
        id: run-llama-3_1-8b-SplashAttention
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3dot1-8b-sa)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3.1-8b \
            model.attention_kernel=splash_attention \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.1 8B (Scan + Offload)
        id: run-llama-3_1-8b-scan-offload
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3dot1-8b-scan-offload)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3.1-8b \
            model/remat=llama-scan-offload \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.0 8B (2D sharding)
        id: run-llama-3-8b-2d
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b-2d)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            model/sharding=llama-fsdp-tp \
            global_batch_size=8 \
            ici_mesh.fsdp=2 \
            ici_mesh.tensor=2 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Mixtral 8x7B
        id: run-mixtral-8x7b
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py mixtral-8x7b)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=mixtral-8x7b \
            model.num_hidden_layers=16 \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.0 8B (2 slice)
        id: run-llama-3-8b-2-slice
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b-2-slice)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run ${{ steps.docker-url-option.outputs.value }} \
            --name $name \
            --num-slices 2 \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            model/sharding=llama-fsdp \
            global_batch_size=16 \
            dcn_mesh.fsdp=2 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

  # Load reference step times
  load-benchmarks:
    name: Load reference step times
    runs-on: ubuntu-24.04
    outputs:
      matrix: ${{ steps.load.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      - name: Load step_time_bounds.yaml
        id: load
        run: |
          # Extract benchmarks as array of objects
          MATRIX=$(yq -o=json -I=0 '.benchmarks | to_entries | map({
            "benchmark": .key,
            "name": .value.name,
            "lower_bound": .value.step_time_lower_bound,
            "upper_bound": .value.step_time_upper_bound
          })' e2e_testing/step_time_bounds.yaml)
          echo "Benchmark matrix JSON: $MATRIX"
          echo "matrix=$MATRIX" >> "$GITHUB_OUTPUT"

  # Validate the results of the workloads
  #
  # Each workload has a step time lower bound and upper bound.
  # The bounds and confidence intervals are programmatically derived from
  # historical E2E test results. To regenerate the bounds, you can run
  # `e2e_testing/update_step_time.py`.
  validate:
    name: ${{ matrix.config.name }}
    needs: [tp-run, load-benchmarks]
    strategy:
      fail-fast: false
      matrix:
        config: ${{ fromJson(needs.load-benchmarks.outputs.matrix) }}
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: >-
        ${{
          matrix.config.benchmark == 'llama-3-8b' && needs.tp-run.outputs.llama-3-8b-name ||
          matrix.config.benchmark == 'llama-3_1-8b-sa' && needs.tp-run.outputs.llama-3_1-8b-sa-name ||
          matrix.config.benchmark == 'llama-3_1-8b-scan-offload' && needs.tp-run.outputs.llama-3_1-8b-scan-offload-name ||
          matrix.config.benchmark == 'llama-3-8b-2d' && needs.tp-run.outputs.llama-3-8b-2d-name ||
          matrix.config.benchmark == 'mixtral-8x7b' && needs.tp-run.outputs.mixtral-8x7b-name ||
          matrix.config.benchmark == 'llama-3-8b-2-slice' && needs.tp-run.outputs.llama-3-8b-2-slice-name
        }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      step_time_lower_bound: ${{ matrix.config.lower_bound }}
      step_time_upper_bound: ${{ matrix.config.upper_bound }}
    secrets: inherit
